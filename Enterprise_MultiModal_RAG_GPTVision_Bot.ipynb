{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Chatbot using Gradio , The Gradio Chatbot asnwers Multimodal (Text and Image) queries from users \n",
    "### The AI Search Index has image embeddings created using the repo https://github.com/mahes-a/ImageIndexing\n",
    "- Create a Gradio Chatbot\n",
    "- Use GPT-4 Turbo with Vision on your data allows the model to generate more customized and targeted answers using Retrieval Augmented Generation based on your own images and image metadata. \n",
    "- Execute the GPT-4 Turbo Chat Completion API with user inputted Image  and text content\n",
    "- The Image is converted into Base64 Data url and passed to Chat Completion API\n",
    "- Chat History is passed to Chat Completion API to answer Multi Modal answers \n",
    "\n",
    "Inspired from Below \n",
    "- Refer to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-image-data\n",
    "- Refer to https://www.gradio.app/guides/multimodal-chatbot-part1\n",
    "- Refer to https://www.analyticsvidhya.com/blog/2023/12/building-a-multimodal-chatbot-with-gemini-and-gradio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports and load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "import gradio as gr\n",
    "import base64\n",
    "import time\n",
    "import os\n",
    "import tqdm as notebook_tqdm\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "  \n",
    "load_dotenv()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Environment Variables and Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Search Index Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gradio Chatbot and execute GPT4V Chat Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set max chat history to keep\n",
    "max_items = 16\n",
    "\n",
    "# Initialize an empty list for the conversation history with max len\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "#OPEN AI KEY\n",
    "GPT4V_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "#Set Headers\n",
    "headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"api-key\": GPT4V_KEY,\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "# Add the system message to the conversation history , Cutomize the System message to your needs\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"You are an AI assistant that helps people find information.Make the URL as markdown hyper links for rendering as hyperlinks example [Gradio Website][1][1]: https://www.gradio.app/\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "#Maintain the conversation_history \n",
    "conversation_history.append(system_message)\n",
    "\n",
    "#RAG Pattern Index details where Images are embedded for Retrieval\n",
    "dataSources = [\n",
    "            {\n",
    "            \"type\": \"AzureCognitiveSearch\",\n",
    "            \"parameters\": {\n",
    "                \"endpoint\": os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "                \"key\": os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "                \"indexName\":os.getenv(\"AZURE_SEARCH_INDEX_NAME\"),\n",
    "                \"fieldsMapping\": {\n",
    "                                \"vectorFields\": \"image_vector\"\n",
    "                            },\n",
    "            }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "#current_dir the avatar image needs to be placed in current_dir\n",
    "current_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "#to keep limited chat history\n",
    "def keep_latest_n_items(history, n):\n",
    "    # Keep only the latest n items\n",
    "    history = history[-n:]\n",
    "    return history\n",
    "\n",
    "#to handle like and dislike for chat responses from LLM, boilerplate code to be expanded\n",
    "def print_like_dislike(x: gr.LikeData):\n",
    "    print(x.index, x.value, x.liked)\n",
    "\n",
    "\n",
    "# Image to Base 64 Converter\n",
    "def convertImageToBase64(image_path):\n",
    "    with open(image_path, 'rb') as img:\n",
    "        encoded_string = base64.b64encode(img.read())\n",
    "    return encoded_string.decode('utf-8')\n",
    "\n",
    "# Function that takes User Inputs and displays it on ChatUI and also maintains the history for chatcompletion\n",
    "def buildHistoryForUiAndChatCompletion(history,txt,img):\n",
    "    #if user enters only text\n",
    "    if not img:\n",
    "        history += [(txt,None)]\n",
    "        # Add the user message to the conversation history\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": txt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        conversation_history.append(user_message)\n",
    "        return history\n",
    "    #if user enters image and text\n",
    "    base64 = convertImageToBase64(img)\n",
    "    data_url = f\"data:image/jpeg;base64,{base64}\"\n",
    "    history += [(f\"{txt} ![]({data_url})\", None)]\n",
    "    # Add the user message to the conversation history\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64}\"\n",
    "                }\n",
    "                },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": txt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    #append the user message to chat api pay load\n",
    "    conversation_history.append(user_message)\n",
    "    return history\n",
    "\n",
    "# Function that takes User Inputs, generates Response and displays on Chat UI\n",
    "def call_AzureOpenAI_Vision_RAG_API(history,text,img):\n",
    "    body = {\n",
    "        \"dataSources\": dataSources,\n",
    "        \"messages\": conversation_history,\n",
    "        \"max_tokens\": 800,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    "    \n",
    "    GPT4V_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")+\"openai/deployments/\"+os.getenv(\"AZURE_OPENAI_VISIONGPT_DEPLOYMENT_NAME\")+\"/extensions/chat/completions?api-version=\"+os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "    #post the API request\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=body)\n",
    "    #print(response.json())\n",
    "    #get llm reponse\n",
    "    content = response.json()['choices'][0]['message']['content']\n",
    " \n",
    "    #llm response added to history\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": content\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    conversation_history.append(assistant_message)\n",
    "    history += [(None,content)]\n",
    "    #conversation_history = keep_latest_n_items(conversation_history, 10)\n",
    "    return history \n",
    "    \n",
    "\n",
    "# Interface Code\n",
    "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"green\", secondary_hue=\"red\")) as app:\n",
    "    with gr.Row():\n",
    "        image_box = gr.Image(type=\"filepath\")\n",
    "        chatbot = gr.Chatbot(\n",
    "            scale = 2,\n",
    "            height=750,\n",
    "            bubble_full_width=False,\n",
    "            #the avv.png is the bot avatar image this needs to be present else comment\n",
    "            avatar_images=(None, (os.path.join(os.path.dirname(current_dir), \"avv.png\")))\n",
    "        )\n",
    "    text_box = gr.Textbox(\n",
    "            placeholder=\"Enter text and press enter\",\n",
    "            container=False,\n",
    "        )\n",
    "    \n",
    "    #btnupd = gr.UploadButton(\"üìÅ\", file_types=[\"image\", \"video\", \"audio\"],type=\"filepath\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clicked = btn.click(buildHistoryForUiAndChatCompletion,\n",
    "                        [chatbot,text_box,image_box],\n",
    "                        chatbot\n",
    "                        ).then(call_AzureOpenAI_Vision_RAG_API,\n",
    "                                [chatbot,text_box,image_box],\n",
    "                                chatbot\n",
    "                                )\n",
    "    chatbot.like(print_like_dislike, None, None)\n",
    "    \n",
    "app.queue()\n",
    "app.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
